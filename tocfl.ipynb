{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00e17d9-146c-494f-afb1-33aefe977942",
   "metadata": {},
   "source": [
    "# TOCFL wordlists parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2894cb-75cb-405a-81af-71a90e15a8bf",
   "metadata": {},
   "source": [
    "Specify source spreadsheet/URL to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5794d3e7-07ce-468e-bdd7-6714600c9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLY_ERRATA=1  # apply corrections from errata.csv?\n",
    "#TOCFL_XLS='https://tocfl.edu.tw/index.php/exam/download'; OUTPUT_CSV='tocfl-latest.csv'  # find latest file automatically\n",
    "TOCFL_XLS='https://tocfl.edu.tw/assets/files/vocabulary/8000zhuyin_202307.zip'; OUTPUT_CSV='tocfl-202307.csv'\n",
    "##TOCFL_XLS='https://tocfl.edu.tw/assets/files/vocabulary/8000zhuyin_202204.zip'; OUTPUT_CSV='tocfl-202204.csv' # same as 202307\n",
    "#TOCFL_XLS=('https://web.archive.org/web/20200227052851/http://www.sc-top.org.tw/download/8000zhuyin.zip', '8000zhuyin_20180419.zip'); OUTPUT_CSV='tocfl-20180419.csv'\n",
    "#TOCFL_XLS=('https://web.archive.org/web/20170621183549/http://www.sc-top.org.tw/download/8000zhuyin.zip', '8000zhuyin_20170324.zip'); OUTPUT_CSV='tocfl-20170324.csv'\n",
    "#TOCFL_XLS=('https://web.archive.org/web/20170223051537/http://www.sc-top.org.tw/download/8000zhuyin.zip', '8000zhuyin_20161230.zip'); OUTPUT_CSV='tocfl-20161230.csv'\n",
    "#TOCFL_XLS=('https://web.archive.org/web/20160818155004/http://www.sc-top.org.tw/download/8000zhuyin.zip', '8000zhuyin_20160316.zip'); OUTPUT_CSV='tocfl-20160316.csv'\n",
    "#TOCFL_XLS=('https://web.archive.org/web/20161215061819/http://www.sc-top.org.tw/download/8000zhuyin.rar', '8000zhuyin_20160215.rar'); OUTPUT_CSV='tocfl-20160215.csv'\n",
    "#TOCFL_XLS='https://web.archive.org/web/20140908011551/http://www.sc-top.org.tw/download/L1-L5vocabualry%20list20111208.xls'; OUTPUT_CSV='top-20111208.csv'\n",
    "#TOCFL_XLS='https://web.archive.org/web/20160116074948/http://www.sc-top.org.tw/download/800+800020100915.xls'; OUTPUT_CSV='top-20100915.csv'\n",
    "#TOCFL_XLS='https://tocfl.edu.tw/assets/files/vocabulary/CCCC_Vocabulary_2022.xls'; OUTPUT_CSV='cccc.csv'\n",
    "##TOCFL_XLS='https://tocfl.edu.tw/assets/files/vocabulary/CCCC_Vocabulary_2017.xls'; OUTPUT_CSV='cccc-2017.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4a9ebe-d428-45e1-b09a-b11e774f7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOCFL_URL=\"https://tocfl.edu.tw/assets/files/vocabulary/8000zhuyin_202307.zip\"\n",
      "TOCFL_XLS=\"downloads/8000zhuyin_202307.xlsx\"\n",
      "OUTPUT_CSV=\"tocfl-202307.csv\"\n"
     ]
    }
   ],
   "source": [
    "!pip install -q opencc genanki\n",
    "import os, re, glob, requests, io, urllib, json, shutil, opencc\n",
    "import pandas as pd\n",
    "import genanki\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "if TOCFL_XLS == 'https://tocfl.edu.tw/index.php/exam/download':\n",
    "    # Find link to latest file automatically\n",
    "    print(f'Downloading {TOCFL_XLS}')\n",
    "    resp = requests.get(TOCFL_XLS).content.decode('utf-8')\n",
    "    urls = re.findall('<a href=\"(/assets/files/vocabulary/8000zhuyin_[0-9]+.zip)\"', resp)\n",
    "    assert len(urls) == 1\n",
    "    TOCFL_XLS = urllib.parse.urljoin(TOCFL_XLS, urls[0])\n",
    "\n",
    "TOCFL_URL = ''\n",
    "if type(TOCFL_XLS) is tuple:\n",
    "    TOCFL_URL, TOCFL_XLS = TOCFL_XLS\n",
    "    TOCFL_XLS = 'downloads/' + TOCFL_XLS\n",
    "elif TOCFL_XLS.startswith('http'):\n",
    "    TOCFL_URL = TOCFL_XLS\n",
    "    TOCFL_XLS = f\"downloads/{os.path.basename(TOCFL_URL)}\"\n",
    "    print(f'\\nTOCFL_URL=\"{TOCFL_URL}\"')\n",
    "\n",
    "if not os.path.exists(TOCFL_XLS):\n",
    "    assert TOCFL_URL != '', f'{TOCFL_XLS} does not exist and is not a URL'\n",
    "    print(f'Downloading {TOCFL_URL} to {TOCFL_XLS}')\n",
    "    ![[ ! -d downloads && -d ../downloads/tocfl ]] && ln -s ../downloads/tocfl downloads\n",
    "    !mkdir -p downloads && curl -o \"{TOCFL_XLS}\" \"{TOCFL_URL}\"\n",
    "    assert os.path.exists(TOCFL_XLS)\n",
    "\n",
    "if TOCFL_XLS.endswith('.zip') or TOCFL_XLS.endswith('.rar'):\n",
    "    TOCFL_ZIP = TOCFL_XLS\n",
    "    if os.path.exists(TOCFL_XLS.replace('.zip', '.xls').replace('.rar', '.xls')):\n",
    "        TOCFL_XLS = TOCFL_XLS.replace('.zip', '.xls').replace('.rar', '.xls')\n",
    "    elif os.path.exists(TOCFL_XLS.replace('.zip', '.xlsx').replace('.rar', '.xlsx')):\n",
    "        TOCFL_XLS = TOCFL_XLS.replace('.zip', '.xlsx').replace('.rar', '.xlsx')\n",
    "    else:\n",
    "        TOCFL_XLS = TOCFL_XLS.replace('.zip', '.xlsx').replace('.rar', '.xlsx')\n",
    "        print(f'Unpacking {TOCFL_ZIP} to {TOCFL_XLS}')\n",
    "        !rm -rf downloads/unpacked && mkdir downloads/unpacked\n",
    "        if TOCFL_ZIP.endswith('.zip'):\n",
    "            !cd downloads/unpacked && unzip \"../{os.path.basename(TOCFL_ZIP)}\"\n",
    "        else:\n",
    "            !cd downloads/unpacked && rar x \"../{os.path.basename(TOCFL_ZIP)}\"\n",
    "        !cp -fv \"$(find downloads/unpacked -name '*.xlsx')\" \"{TOCFL_XLS}\"\n",
    "        !rm -rf downloads/unpacked\n",
    "        assert os.path.exists(TOCFL_XLS)\n",
    "        !echo; ls -l \"{TOCFL_XLS}\"; sha256sum \"{TOCFL_XLS}\"; chmod a-w \"{TOCFL_XLS}\"\n",
    "    del TOCFL_ZIP\n",
    "\n",
    "print(f'TOCFL_XLS=\"{TOCFL_XLS}\"')\n",
    "print(f'OUTPUT_CSV=\"{OUTPUT_CSV}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76375b0f-2383-4038-8f0f-17b5dcfad024",
   "metadata": {},
   "source": [
    "### Parse and cleanup .xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4373f6e-6c4d-4f1c-be38-84316f57e27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing downloads/8000zhuyin_202307.xlsx\n",
      "e979ac6d953fb493502e54536b4b6ff534d06e3938700052aa15806d514efc92  downloads/8000zhuyin_202307.xlsx\n",
      "Sheet 1: 準備級一級(Novice 1)\tL0-1\t160 rows\n",
      "Sheet 2: 準備級二級(Novice 2)\tL0-2\t234 rows\n",
      "Sheet 3: 入門級(Level 1)   \tL1\t347 rows\n",
      "Sheet 4: 基礎級(Level 2)   \tL2\t485 rows\n",
      "Sheet 5: 進階級(Level 3)   \tL3\t1173 rows\n",
      "Sheet 6: 高階級(Level 4)   \tL4\t2342 rows\n",
      "Sheet 7: 流利級(Level 5)   \tL5\t2776 rows\n",
      "Total: 7517\n",
      "\n",
      "各等詞條數(Entry Number)\n",
      "  Unnamed: 0 準備1級 準備2級  入門級   基礎級   進階級   高階級   流利級    總計\n",
      "0      各等詞彙量  160  234  347   485  1173  2342  2776  7517\n",
      "1      累計詞彙量       394  741  1226  2399  4741  7517      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Pinyin</th>\n",
       "      <th>POS</th>\n",
       "      <th>Level</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1226</td>\n",
       "      <td>7517</td>\n",
       "      <td>7517</td>\n",
       "      <td>7517</td>\n",
       "      <td>7517</td>\n",
       "      <td>7517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>12</td>\n",
       "      <td>7189</td>\n",
       "      <td>6780</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "      <td>7517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>個人資料</td>\n",
       "      <td>中</td>\n",
       "      <td>jí</td>\n",
       "      <td>N</td>\n",
       "      <td>L5</td>\n",
       "      <td>L0-1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2985</td>\n",
       "      <td>2776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Context Vocabulary Pinyin   POS Level       ID\n",
       "count     1226       7517   7517  7517  7517     7517\n",
       "unique      12       7189   6780   104     7     7517\n",
       "top       個人資料          中    jí      N    L5  L0-1001\n",
       "freq       220          3      7  2985  2776        1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEVELS_MP = {\n",
    "    # 8000zhuyin_202204.xlsx, 8000zhuyin_202307.xlsx\n",
    "    'Novice 1': 'L0-1',\n",
    "    'Novice 2': 'L0-2',\n",
    "    'Level 1': 'L1',\n",
    "    'Level 2': 'L2',\n",
    "    'Level 3': 'L3',\n",
    "    'Level 4': 'L4',\n",
    "    'Level 5': 'L5',\n",
    "\n",
    "    # 20180419\n",
    "    '準備級一級': 'L0-1',\n",
    "    '準備級二級': 'L0-2',\n",
    "    '入門級': 'L1',\n",
    "    '基礎級': 'L2',\n",
    "    '進階級': 'L3',\n",
    "    '高階級': 'L4',\n",
    "    '流利級': 'L5',\n",
    "\n",
    "    # 20161230\n",
    "    '準備一級': 'L0-1',\n",
    "    '準備二級': 'L0-2',\n",
    "\n",
    "    # SC-TOP 800.xls\n",
    "    '基礎': 'L2', # Band A / L1-L2\n",
    "    '初等': 'L3',\n",
    "    '初等（舊）': 'drop-L3',\n",
    "    '中等': 'L4',\n",
    "    '高等': 'L5',\n",
    "\n",
    "    # CCCC\n",
    "    '萌芽級': 'L1',\n",
    "    '成長級': 'L2',\n",
    "    '茁壯級': 'L3',\n",
    "    '三級詞彙': 'drop-vocab',\n",
    "    '詞表說明': 'drop-expl',\n",
    "    '詞性縮寫對照表': 'drop-pos',\n",
    "}\n",
    "\n",
    "print(f'Parsing {TOCFL_XLS}')\n",
    "!sha256sum {TOCFL_XLS}\n",
    "\n",
    "xls = pd.ExcelFile(TOCFL_XLS)\n",
    "sheets = {}\n",
    "\n",
    "for i, name in enumerate(xls.sheet_names):\n",
    "    if OUTPUT_CSV.startswith('cccc'):\n",
    "        df = xls.parse(name, dtype='str', skiprows=1).fillna('')\n",
    "    else:\n",
    "        df = xls.parse(name, dtype='str').fillna('')\n",
    "\n",
    "    if 'Entry Number' in name or '各等詞條數' in name:\n",
    "        break\n",
    "\n",
    "    if name in LEVELS_MP:\n",
    "        level = LEVELS_MP[name]\n",
    "    else:\n",
    "        level = LEVELS_MP[re.findall('[(](.*)[)]', name)[0]]\n",
    "    if level.startswith('drop-'):\n",
    "        print(f'Sheet {i+1}: {name:<15}\\t{level}\\t{len(df)} rows - ignoring')\n",
    "        continue\n",
    "    print(f'Sheet {i+1}: {name:<15}\\t{level}\\t{len(df)} rows')\n",
    "\n",
    "    df = df.rename(columns=lambda s: s.strip().split('\\n')[-1])\n",
    "    df = df.rename(columns={\n",
    "        'Parts of Speech': 'POS',\n",
    "        '詞類': 'POS',\n",
    "        '詞彙': 'Vocabulary',\n",
    "        '漢語拼音': 'Pinyin',\n",
    "        '拼音': 'Pinyin',\n",
    "        '注音': 'Zhuyin',\n",
    "        '編號': 'drop-No1',\n",
    "        '拼音排序': 'drop-No2',\n",
    "        '等級': 'drop-Level',\n",
    "        '英文解釋': 'Meaning',\n",
    "        '任務領域': 'Category',\n",
    "\n",
    "        # CCCC\n",
    "        '分類': 'Category',\n",
    "        '細目': 'Subcategory',\n",
    "        '正體字': 'Vocabulary',\n",
    "        '简体字': 'Simplified',\n",
    "        '漢拼': 'Pinyin',\n",
    "        '詞性': 'POS',\n",
    "        '英文': 'Meaning',\n",
    "    })\n",
    "    df['Level'] = level\n",
    "    df = df[df.Vocabulary.fillna('') != ''].copy()\n",
    "    df = df[~df.Vocabulary.str.match('^[A-Z]$')].copy()\n",
    "\n",
    "    if OUTPUT_CSV.startswith('cccc'):\n",
    "        df['ID'] = ['%s-%.3d' % (level, i+1) for i in range(len(df))]\n",
    "    elif level.startswith('L0'):\n",
    "        df['ID'] = ['%s%.3d' % (level, i+1) for i in range(len(df))]\n",
    "    else:\n",
    "        df['ID'] = ['%s-%.4d' % (level, i+1) for i in range(len(df))]\n",
    "    for col in list(df):  # 800.xls\n",
    "        if col.startswith('drop-') or repr(sorted(set(df[col]))) in [\"['', '15']\", \"['', 'pī ']\", \"['', '匹_1']\"]:\n",
    "            df = df.drop(columns=[col])\n",
    "    sheets[level] = df\n",
    "\n",
    "excel_df = pd.concat(sheets.values())\n",
    "print(f'Total: {len(excel_df)}')\n",
    "\n",
    "if OUTPUT_CSV.startswith('tocfl'):\n",
    "    print(f'\\n{name}\\n%s' % str(df))\n",
    "    assert str(len(excel_df)) in str(df)\n",
    "\n",
    "excel_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867895c5-3e95-4e44-8fa8-4d0cb2feb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "opencc_tw2s = opencc.OpenCC('tw2s')\n",
    "\n",
    "tgh_level = None\n",
    "if not os.path.exists('../chars/tgh.csv'):\n",
    "    print(\"../chars/tgh.csv doesn't exist - skipping verification against it\")\n",
    "else:\n",
    "    # Character levels from Table of General Standard Chinese Characters for verification.\n",
    "    tgh_level = pd.read_csv('../chars/tgh.csv').set_index('char').level.to_dict()\n",
    "\n",
    "# Convert to simplified characters + verify\n",
    "def to_simplified(trad):\n",
    "    if trad == '什麼/甚麼': return '什么'\n",
    "    if trad == '甚麼': return '什么'\n",
    "    simp = opencc_tw2s.convert(trad)\n",
    "    for x, y in ('擡抬', '砲炮', '牠它', '艶艳', '妳你'):\n",
    "        simp = simp.replace(x, y)\n",
    "    if '/' in simp and len(set(simp.split('/'))) == 1:\n",
    "        simp = simp.split('/')[0]\n",
    "    if tgh_level:\n",
    "        for c in simp:\n",
    "            assert tgh_level.get(c, 9) <= 2 or c in '/(),', (trad, simp, c, tgh_level.get(c))\n",
    "    return simp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf860496-b44d-4ec8-a9d4-c4c6d3c3c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Pinyin column\n",
    "\n",
    "pinyin_corr_df = pd.read_csv('errata.csv', comment='#', dtype='str')\n",
    "\n",
    "def fix_pinyin(py, trad=''):\n",
    "    for x, y in ['ăǎ', 'ŏǒ', 'ĭǐ', 'ŭǔ', 'ɑa', '；/', '（(', '）)', \n",
    "                 ('\\u200b', ''), (' +[)]', ')'), (' */ *', '/'), (r'\\s+', ' '),\n",
    "                 ('; ', '/'), ('nǔ:', 'nǚ'),\n",
    "                ]:\n",
    "        py = re.sub(x, y, py).strip()\n",
    "\n",
    "    if APPLY_ERRATA:\n",
    "        for row in pinyin_corr_df.itertuples():\n",
    "            if row.Pinyin == py and row.Traditional == trad:\n",
    "                py = row.Corrected\n",
    "    py = py.strip()\n",
    "    assert re.match('^[a-zāáǎàēéěèīíǐìōóǒòūúǔùüǘǚǜ/(), \\']+$', py.lower()), (py, repr(py))\n",
    "    return py\n",
    "\n",
    "assert fix_pinyin('xiăo') == 'xiǎo'   # a c -> a v\n",
    "assert fix_pinyin('chuāng(zi )/chuānghu') == 'chuāng(zi)/chuānghu'\n",
    "assert fix_pinyin('liànài', trad='戀愛') == (\"liàn'ài\" if APPLY_ERRATA else 'liànài')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2673a3e9-88ab-4cff-89e0-35baadfff546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Vocabulary column (hanzi)\n",
    "\n",
    "def fix_vocabulary(s):\n",
    "    for x, y in [\n",
    "        ('\\u200b', ''), ('（', '('), ('）', ')'), (' */ *', '/'), (r'\\s+', ' '),\n",
    "        ('[(]面˙ㄇㄧㄢ[)]', '(面)'), ('、', ','),\n",
    "        (r'^([\\u4E00-\\u9FFF]{2,3})\\[([\\u4E00-\\u9FFF]{2,3})\\]$', r'\\1/\\2'), #CCCC simplified\n",
    "    ]:\n",
    "        s = re.sub(x, y, s).strip()\n",
    "\n",
    "    # Zhuyin hints are reduntant with pinyin and not very relevant for foreigners, drop them\n",
    "    # Take care to not drop (一) in 差(一)... etc\n",
    "    s = s.replace('(˙ㄇ一)', '(˙ㄇㄧ)')\n",
    "    s = s.replace('(一˙)', '(ㄧ˙)')\n",
    "    s = re.sub(r'[(（][ㄅㄈㄉㄊㄋㄍㄎㄏㄐㄑㄒㄓㄕㄗㄙㄚㄛㄞㄌㄟㄠㄡㄢㄣㄤㄧㄨㄩㄇㄝㄆㄌㄨㄥㄔㄘㄖˊˋ˙\\uf8f8]+[)）]', '', s).strip()\n",
    "\n",
    "    if len(s) == 2 and s[-1] in '1234': s = s[0]  # 丟1 叫4\n",
    "    if len(s) == 3 and s[-2] == '_' and s[-1] in '1234': s = s[0]\n",
    "\n",
    "    assert re.match('^[\\u4E00-\\u9FFF/(),]+$', s), (row, s)\n",
    "    return s\n",
    "\n",
    "assert fix_vocabulary('名字(˙ㄗ)') == '名字'\n",
    "assert fix_vocabulary('差(一)點(兒)') == '差(一)點(兒)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aebee411-4190-4948-b401-b8a35ea0def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiguous variant entries - manually disambiguated\n",
    "# Variants column format here:\n",
    "#  trad [pinyin] / ...\n",
    "#  trad [pinyin] (POS) / ...\n",
    "variants_exc_txt = '''\n",
    "Vocabulary,Pinyin,POS,Variants\n",
    "盒/盒(子),hé/hézi,M / N,盒 [hé] (M) / 盒子 [hézi] (N)\n",
    "盤/盤(子),pán(zi),M / N,盤 [pán] (M) / 盤子 [pánzi] (N)\n",
    "瓶/瓶(子),píng(zi),M / N,瓶 [píng] (M) / 瓶子 [píngzi] (N)\n",
    "箱/箱(子),xiāng/xiāng(zi),M / N,箱 [xiāng] (M) / 箱子 [xiāngzi] (N)\n",
    "葉(子),yè(zi),M / N,葉 [yè] / 葉子 [yèzi]\n",
    "姊姊/姐姐/姊/姐,jiějie/jiě,N,姊姊 [jiějie] / 姐姐 [jiějie] / 姊 [jiě] / 姐 [jiě]\n",
    "這裡/這裏/這兒,zhèlǐ/zhèr,N,這裡 [zhèlǐ] / 這裏 [zhèlǐ] / 這兒 [zhèr]\n",
    "那裡/那裏/那兒,nàlǐ/nàr,N,那裡 [nàlǐ] / 那裏 [nàlǐ] / 那兒 [nàr]\n",
    "哪裡/哪裏/哪兒,nǎlǐ/nǎr,N,哪裡 [nǎlǐ] / 哪裏 [nǎlǐ] / 哪兒 [nǎr]\n",
    "畫/畫兒,huà/huàr,V / N,畫 [huà] / 畫兒 [huàr]\n",
    "計畫/計劃,jìhuà,V / N,計畫 [jìhuà] / 計劃 [jìhuà]\n",
    "手錶/手表/錶/表,shǒubiǎo/biǎo,N,手錶 [shǒubiǎo] / 手表 [shǒubiǎo] / 錶 [biǎo] / 表 [biǎo]\n",
    "刷(子)/刷,shuā(zi)/shuā,N / V,刷 [shuā] (N) / 刷子 [shuāzi] (N) / 刷 [shuā] (V)\n",
    "保證/証,bǎozhèng,N / V,保證 [bǎozhèng] / 保証 [bǎozhèng]\n",
    "規畫/規劃,guīhuà,N / V,規畫 [guīhuà] / 規劃 [guīhuà]\n",
    "駕駛/駕,jiàshǐ/jià,N / V,駕駛 [jiàshǐ] / 駕 [jià]\n",
    "黏/粘,nián,V / Vs,黏 [nián] / 粘 [nián]\n",
    "占/佔,zhàn,V / Vst,占 [zhàn] / 佔 [zhàn]\n",
    "架/架(子),jià/jià(zi),V / N,架 [jià] (V) / 架子 [jiàzi] (N)\n",
    "以至/以致/以至於/以致於,yǐzhì/yǐzhìyú,Conj,以至 [yǐzhì] / 以致 [yǐzhì] / 以至於 [yǐzhìyú] / 以致於 [yǐzhìyú]\n",
    "嘴唇/嘴脣/唇/脣,zuǐchún/chún,N,嘴唇 [zuǐchún] / 嘴脣 [zuǐchún] / 唇 [chún] / 脣 [chún]\n",
    "聲音/聲,shēngyīn,N,聲音 [shēngyīn] / 聲 [shēng]\n",
    "訪問/訪,fǎngwèn,V,訪問 [fǎngwèn] / 訪 [fǎng]\n",
    "舅舅/舅,jiùjiu,N,舅舅 [jiùjiu] / 舅 [jiù]\n",
    "漸漸/漸,jiànjiàn,Adv,漸漸 [jiànjiàn] / 漸 [jiàn]\n",
    "夜裡/裏,yèlǐ/lǐ,N,夜裡 [yèlǐ] / 夜裏 [yèlǐ]\n",
    "默默/默,mòmò,Adv,默默 [mòmò] / 默 [mò]\n",
    "偶而/偶爾,ǒuér,Adv,偶而 [ǒu'ér] / 偶爾 [ǒu'ěr]\n",
    "\n",
    "部分/部份,bùfen,,部分 [bùfen] / 部份 [bùfen]\n",
    "部分/部份,bùfèn,,部分 [bùfèn] / 部份 [bùfèn]\n",
    "部分/部份,bùfen / bùfèn,,部分 [bùfen] / 部份 [bùfèn]\n",
    "部分/份,bùfen / bùfèn,,部分 [bùfen] / 部份 [bùfèn]\n",
    "差不多,chàbuduō / chābùduō,,差不多 [chàbuduō] / 差不多 [chābùduō]\n",
    "\n",
    "# older tocfl\n",
    "一點/一點兒,yīdiǎn(r),Det,一點 [yīdiǎn] / 一點兒 [yīdiǎnr]\n",
    "窗/窗戶,chuānghu,N,窗 [chuāng] / 窗戶 [chuānghu]\n",
    "差點/差點兒,chà(yī)diǎn/chà(yī)diǎnr,Adv,差點 [chàdiǎn] / 差點兒 [chàdiǎnr]\n",
    "有點/有點兒,yǒu(yì)diǎn/yǒu(yì)diǎnr,Adv,有點 [yǒudiǎn] / 有點兒 [yǒudiǎnr]\n",
    "以至/致(於),yǐzhì(yú),Conj,以至 [yǐzhì] / 以致 [yǐzhì] / 以至於 [yǐzhìyú] / 以致於 [yǐzhìyú]\n",
    "凡/凡是,fánshì,Conj,凡 [fán] / 凡是 [fánshì]\n",
    "偶而/爾,ǒuér/ěr,Adv,偶而 [ǒu'ér] / 偶爾 [ǒu'ěr]\n",
    "角色,jiǎo/juésè,N,角色 [jiǎosè] / 角色 [juésè]\n",
    "主角,zhǔjiǎo/jué,N,主角 [zhǔjiǎo] / 主角 [zhǔjué]\n",
    "外(面),wàimiàn,N,外 [wài] / 外面 [wàimiàn]\n",
    "窗/窗戶,chuānghù,N,窗 [chuāng] / 窗戶 [chuānghù]\n",
    "沒有,méi(yǒu),,沒 [méi] / 沒有 [méiyǒu]\n",
    "公共汽車(公車),gōnggòngqìchē(gōngchē),,公共汽車 [gōnggòngqìchē] / 公車 [gōngchē]\n",
    "角/角色,jiǎo/juésè,,角色 [jiǎosè] / 角色 [juésè]\n",
    "主角/角,zhǔjiǎo/jué,,主角 [zhǔjiǎo] / 主角 [zhǔjué]\n",
    "\n",
    "# cccc\n",
    "\"災(火災,水災)\",\"zāi(huǒzāi, shuǐzāi)\",N,災 [zāi] / 火災 [huǒzāi] / 水災 [shuǐzāi]\n",
    "星期天/星期日,xīngqítiān/xīngqītiān/xīngqírì/xīngqīrì,N/ADV,星期天 [xīngqítiān] / 星期天 [xīngqītiān] / 星期日 [xīngqírì] / 星期日 [xīngqīrì]\n",
    "星期天/星期日,Xīngqítiān/Xīngqītiān/Xīngqírì/Xīngqīrì,N/ADV,星期天 [Xīngqítiān] / 星期天 [Xīngqītiān] / 星期日 [Xīngqírì] / 星期日 [Xīngqīrì]\n",
    "圓/圓形,yuán/yuánxíng,VS/N,圓 [yuán] / 圓形 [yuánxíng]\n",
    "杯(子),bēi(zi),N/M,杯 [bēi] (M) / 杯子 [bēizi] (N)\n",
    "運動,yùndòng/yùndong,N/V,運動 [yùndòng] / 運動 [yùndong]\n",
    "畫/畫兒,huà/huàr,V/N,畫 [huà] / 畫兒 [huàr]\n",
    "和,hé/hàn,Prep/Conj,和 [hé] / 和 [hàn]\n",
    "事/事情,shì/shìqíng/shìqing,N,事 [shì] / 事情 [shìqíng] / 事情 [shìqing]\n",
    "盒(子),hé(zi),N/M,盒 [hé] (M) / 盒子 [hézi] (N)\n",
    "盤(子),pán(zhi),N/M,盤 [pán] (M) / 盤子 [pánzi] (N)\n",
    "有時/有時候,yǒushí/yǒushíhòu/yǒushíhou,ADV,有時 [yǒushí] / 有時候 [yǒushíhòu] / 有時候 [yǒushíhou]\n",
    "廣播,guǎngbò/guǎngbō,N/V,廣播 [guǎngbò] / 廣播 [guǎngbō]\n",
    "椰(子),yézi,N,椰 [yé] / 椰子 [yézi]\n",
    "\n",
    "# top\n",
    "點(兒),diǎn(ér),M/N,點 [diǎn] / 點兒 [diǎnr]\n",
    "男孩(兒)(子),nánhái(ér)(zi),N,男孩 [nánhái] / 男孩兒 [nánháir] / 男孩子 [nánháizi]\n",
    "女孩(兒)(子),nǚhái(ér)(zi),N,女孩 [nǚhái] / 女孩兒 [nǚháir] / 女孩子 [nǚháizi]\n",
    "差(一)點(兒),chà(yì)diǎn(ér),Adv,差一點 [chàyīdiǎn] / 差一點兒 [chàyīdiǎnr] / 差點 [chàdiǎn] / 差點兒 [chàdiǎnr]\n",
    "哪裡/裏(兒),nǎlǐ/er,Adv,哪裡 [nǎlǐ] / 哪裏 [nǎlǐ] / 哪兒 [nǎr]\n",
    "男孩(子/兒),nánhái(zi/ér),N,男孩 [nánhái] / 男孩兒 [nánháir] / 男孩子 [nánháizi]\n",
    "女孩(子/兒),nǚhái(zi/ér),N,女孩 [nǚhái] / 女孩兒 [nǚháir] / 女孩子 [nǚháizi]\n",
    "小孩(子/兒),xiǎohái(zi/ér),N,小孩 [xiǎohái] / 小孩兒 [xiǎoháir] / 小孩子 [xiǎoháizi]\n",
    "老頭(兒/子),lǎotóu(ér/zi),N,老頭 [lǎotóu] / 老頭兒 [lǎotóur] / 老頭子 [lǎotóuzi]\n",
    "偶而/爾,ǒuér(ěr),Adv,偶而 [ǒu'ér] / 偶爾 [ǒu'ěr]\n",
    "不至/致於,búzhìyú,Adv,不至於 [bùzhìyú] / 不致於 [bùzhìyú]\n",
    "裡面,lǐ(miàn),N,裡 [lǐ] / 裡面 [lǐmiàn]\n",
    "謝,xiè/xièxie,V,謝 [xiè] / 謝謝 [xièxie]\n",
    "謝謝,xiè/xièxie,V,謝謝 [xièxie] / 謝 [xiè]\n",
    "哪裡/兒,nǎlǐ/ér,,哪裡 [nǎlǐ] / 哪兒 [nǎr]\n",
    "那裡/兒,nàlǐ/ér,,那裡 [nàlǐ] / 那兒 [nàr]\n",
    "這裡/兒,zhèlǐ/ér,,這裡 [zhèlǐ] / 這兒 [zhèr]\n",
    "那麼(樣),nàme(yàng),,那麼 [nàme] / 那樣 [nàyàng] / 那麼樣 [nàmeyàng]\n",
    "這麼(樣),zhème(yàng),,這麼 [zhème] / 這樣 [zhèyàng] / 這麼樣 [zhèmeyàng]\n",
    "部分/份,bùfèn/fèn,,部分 [bùfèn] / 部份 [bùfèn]\n",
    "大部分/份,dàbùfèn/fèn,,大部分 [dàbùfèn] / 大部份 [dàbùfèn]\n",
    "老闆/板,lǎobǎn/bǎn,,老闆 [lǎobǎn] / 老板 [lǎobǎn]\n",
    "哪裡/兒,nǎlǐ/ér,,哪裡 [nǎlǐ] / 哪兒 [nǎr]\n",
    "其他/它,qítā/tā,,其他 [qítā] / 其它 [qítā]\n",
    "夜裡/裏,yèlǐ/lǐ,,夜裡 [yèlǐ] / 夜裏 [yèlǐ]\n",
    "一下子/兒,yíxiàzi/ér,,一下子 [yīxiàzi] / 一下兒 [yīxiàr]\n",
    "似的/地,sìde/di,,似的 [sìde] / 似地 [sìdi]\n",
    "計畫/劃,jìhuà/huà,,計畫 [jìhuà] / 計劃 [jìhuà]\n",
    "'''\n",
    "variants_exc_df = pd.read_csv(io.StringIO(variants_exc_txt), comment='#', dtype='str').fillna('')\n",
    "variants_exc_mp = variants_exc_df.set_index(['Vocabulary', 'Pinyin', 'POS']).Variants.to_dict()\n",
    "\n",
    "# Some variants in TOCFL are specified as \"...x/y...\" character pairs. Valid pairs here:\n",
    "variant_pairs = [\n",
    "    '做作', '布佈', '嘗嚐', '溼濕', '分份', '畫劃', '裡裏', '秘祕', '台臺', '周週',\n",
    "    '汙污', '消宵', '占佔', '證証', '雇僱', '迴回', '剎煞', '的地', '艶豔', '嘆歎',\n",
    "    '連聯', '秘祕', '伙夥', '闆板', '煙菸', '至致', '蹟跡', '畫劃', '分份', '份分',\n",
    "    '他它', '裡裏', '證証', '溼濕', '煙菸'\n",
    "]\n",
    "\n",
    "def get_variants(vocab, pinyin, pos) -> str:\n",
    "    \"\"\"Returns disambiguated list of variants from TOCFL's vocab+pinyin+pos strings.\"\"\"\n",
    "\n",
    "    pos = pos.strip('()')\n",
    "\n",
    "    res = variants_exc_mp.get((vocab, pinyin, pos))\n",
    "    if res:\n",
    "        return res.strip()\n",
    "    res = variants_exc_mp.get((vocab, pinyin, ''))\n",
    "    if res:\n",
    "        return res.strip()\n",
    "\n",
    "    ps = re.sub('[^()/,]', '', pinyin)\n",
    "    vs = re.sub('[^()/,]', '', vocab)\n",
    "    if ps == '' and vs == '':\n",
    "        return ''\n",
    "\n",
    "    if ps == '' and vs == '/':\n",
    "        for x, y in variant_pairs:\n",
    "            if not (f'{x}/{y}' in vocab or f'{y}/{x}' in vocab): continue\n",
    "            if f'{y}/{x}' in vocab:\n",
    "                x, y = y, x\n",
    "            assert f'{x}/{y}' in vocab\n",
    "            vx = vocab.replace(f'{x}/{y}', x)\n",
    "            vy = vocab.replace(f'{x}/{y}', y)\n",
    "            return f'{vx} [{pinyin}] / {vy} [{pinyin}]'\n",
    "\n",
    "    if (ps != vs and ps != '' and vs != '') or \\\n",
    "       (ps+vs != '' and '/' in pos and pos not in ('N/M', 'M/N')) or \\\n",
    "       (ps == '' and vs != '' and len(set(map(len, vocab.split('/')))) != 1):\n",
    "        print('ps=%s vs=%s' % (ps, vs))\n",
    "        raise Exception('Ambiguous entry: %s' % ','.join([vocab, pinyin, pos, vocab]))\n",
    "        print('%s' % ','.join([vocab, pinyin, pos, vocab]))\n",
    "        return ''\n",
    "\n",
    "    if ps != '' and vs == '':\n",
    "        assert '/' not in pos\n",
    "        assert ps == '/'\n",
    "        res = ' / '.join([f'{vocab} [{p.strip()}]' for p in pinyin.split('/')])\n",
    "        return res\n",
    "    if ps == '' and vs != '':\n",
    "        assert '/' not in pos\n",
    "        res = vocab.split('/')\n",
    "        assert len(set(map(len, res))) == 1, vocab\n",
    "        res = ' / '.join([f'{s} [{pinyin}]' for s in res])\n",
    "        return res\n",
    "    assert ps == vs\n",
    "\n",
    "    res = []\n",
    "    for vo, py in zip(vocab.split('/'), pinyin.split('/')):\n",
    "        assert vo.count('(') == py.count('(') and vo.count('(') <= 1\n",
    "        assert vo.count(')') == py.count(')') and vo.count(')') <= 1\n",
    "        assert vo.count('(') == vo.count(')')\n",
    "        if '(' in vo:\n",
    "            vm = re.match('^([^() ]*) *[(]([^() ]+)[)] *([^() ]*)$', vo)\n",
    "            assert vm, vo\n",
    "            pm = re.match('^([^() ]*) *[(]([^() ]+)[)] *([^() ]*)$', py)\n",
    "            assert pm, py\n",
    "\n",
    "            if vm[2] == '兒' and vm[3] == '' and pm[2] == 'ér' and pm[3] == '':\n",
    "                pm = [pm[0], pm[1], 'r', '']\n",
    "\n",
    "            res.append(f'{vm[1]}{vm[3]} [{pm[1]}{pm[3]}]')\n",
    "            res.append(f'{vm[1]}{vm[2]}{vm[3]} [{pm[1]}{pm[2]}{pm[3]}]')\n",
    "        else:\n",
    "            res.append(f'{vo} [{py}]')\n",
    "\n",
    "    if pos in ('N/M', 'M/N'):\n",
    "        assert len(res) == 2\n",
    "        res.sort(key=lambda s: len(s))\n",
    "        assert '子' not in res[0] and '子' in res[1]\n",
    "        res[0] += ' (M)'\n",
    "        res[1] += ' (N)'\n",
    "\n",
    "    w = [x.split()[0] for x in res]\n",
    "    if len(w) == 2 and len(w[0]) == 1 and w[1] == w[0] + '子' and pos != 'M/N':\n",
    "        # reorder 房子 / 房\n",
    "        res[0], res[1] = res[1], res[0]\n",
    "        #print(res)\n",
    "    if len(w) == 2 and len(w[0]) == 1 and w[1] == w[0] + w[0] and w[0] in ('弟哥姐姊爸媽'):\n",
    "        res[0], res[1] = res[1], res[0]\n",
    "        #print(res)\n",
    "\n",
    "    res = ' / '.join(res)\n",
    "    return res\n",
    "\n",
    "def variants_to_json(variants):\n",
    "    if not variants or variants != variants:\n",
    "        return ''\n",
    "    arr = []\n",
    "    for var in variants.split(' / '):\n",
    "        m = re.match(r'^([^ ()\\[\\]]+) \\[([^()\\[\\]]+)\\](?:$| [(]([A-Z]+)[)])$', var)\n",
    "        assert m, variants\n",
    "        var = {\n",
    "            'Traditional': m[1],\n",
    "            'Simplified': to_simplified(m[1]),\n",
    "            'Pinyin': fix_pinyin(m[2], m[1]),\n",
    "        }\n",
    "        if m[3]:\n",
    "            var['POS'] = m[3]\n",
    "        arr.append(var)\n",
    "    return json.dumps(arr, ensure_ascii=False)\n",
    "\n",
    "assert get_variants('台灣/臺灣', 'táiwān', 'N') == '台灣 [táiwān] / 臺灣 [táiwān]'\n",
    "assert get_variants('小孩(子)', 'xiăohái(zi)', 'N') == '小孩 [xiăohái] / 小孩子 [xiăoháizi]'\n",
    "assert get_variants('公共汽車/公車', 'gōnggòngqìchē/gōngchē', 'N') == '公共汽車 [gōnggòngqìchē] / 公車 [gōngchē]'\n",
    "assert get_variants('公共汽車(公車)', 'gōnggòngqìchē(gōngchē)', 'N') == '公共汽車 [gōnggòngqìchē] / 公車 [gōngchē]'\n",
    "assert get_variants('盒/盒(子)', 'hé/hézi', 'M / N') == '盒 [hé] (M) / 盒子 [hézi] (N)'\n",
    "assert get_variants('差(一)點/差(一)點兒', 'chà(yī)diǎn/chà(yī)diǎnr','Adv') == \\\n",
    "                    '差點 [chàdiǎn] / 差一點 [chàyīdiǎn] / 差點兒 [chàdiǎnr] / 差一點兒 [chàyīdiǎnr]'\n",
    "assert get_variants('角色', 'jiǎo/juésè', 'N') == '角色 [jiǎosè] / 角色 [juésè]' #XXX\n",
    "assert get_variants('計畫/劃', 'jìhuà', 'V') == '計畫 [jìhuà] / 計劃 [jìhuà]'\n",
    "assert get_variants('(老)鼠', '(lǎo)shǔ', 'N') == '鼠 [shǔ] / 老鼠 [lǎoshǔ]'\n",
    "assert get_variants('鼻(子)', 'bí(zi)', 'N') == '鼻子 [bízi] / 鼻 [bí]'\n",
    "assert get_variants('瓶(子)', 'píng(zi)', 'M/N') == '瓶 [píng] (M) / 瓶子 [píngzi] (N)'\n",
    "assert get_variants('唱歌(兒)', 'chànggē(ér)', 'V') == '唱歌 [chànggē] / 唱歌兒 [chànggēr]'\n",
    "\n",
    "assert (variants_to_json('台灣 [Táiwān] / 臺灣 [Táiwān]') ==\n",
    "        '[{\"Traditional\": \"台灣\", \"Simplified\": \"台湾\", \"Pinyin\": \"Táiwān\"}, ' +\n",
    "        '{\"Traditional\": \"臺灣\", \"Simplified\": \"台湾\", \"Pinyin\": \"Táiwān\"}]')\n",
    "assert (variants_to_json('盤 [pán] (M) / 盤子 [pánzi] (N)') == \n",
    "        '[{\"Traditional\": \"盤\", \"Simplified\": \"盘\", \"Pinyin\": \"pán\", \"POS\": \"M\"}, ' +\n",
    "        '{\"Traditional\": \"盤子\", \"Simplified\": \"盘子\", \"Pinyin\": \"pánzi\", \"POS\": \"N\"}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15410345-32ff-4545-ac70-40227c7b252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_meaning(text):\n",
    "    text = text.strip()\n",
    "    for x, y in [\n",
    "        (r'\\s+', ' '),\n",
    "        (r'([^ ])/ ', r'\\1/'),\n",
    "        (r'([^ ])([:;,\\]])([^ ])', r'\\1\\2 \\3'),\n",
    "        (r'([^ ])([(])([^ )]+[ ,/;:.])', r'\\1 \\2\\3'),\n",
    "        (r'([^ ])([(])([0-9]+[)])', r'\\1 \\2\\3'),\n",
    "        (r' [)] ', ') '), (r' [)]$', ')'), (r' \\( ', ' ('), (r'^\\( ', '('),\n",
    "        (r' [)], ', '), '),\n",
    "        (' ; ', '; '), (' , ', ', '),\n",
    "        ('…+', '...'), (' [.]', '.'),\n",
    "        (r'\\.etc', '. etc'),\n",
    "        (r'\\.[(]', '. ('),\n",
    "        (r'something[.]+', 'sth.'),\n",
    "        (r'someone[.]+', 'sb.'),\n",
    "        (r'something', 'sth'),\n",
    "        (r'someone', 'sb'),\n",
    "        (r'etc[.]+[)]', 'etc)'),\n",
    "        (r'etc[.]+$', 'etc'),\n",
    "        (r'(,|[.][.][.]) etc', ' etc'),\n",
    "        (' : ', ': '),\n",
    "        (' *[;,]$', ''),\n",
    "        (r'\\(1\\) *', '①'),\n",
    "        (r'\\(2\\) *', '②'),\n",
    "        (r'\\(3\\) *', '③'),\n",
    "        (r'\\(4\\) *', '④'),\n",
    "        (r'\\(5\\) *', '⑤'),\n",
    "        ('^Measure word', 'measure word'),\n",
    "        ('^Partial measure word', 'partial measure word'),\n",
    "        ('^Collective measure word', 'collective measure word'),\n",
    "        ('^Coi?ntainer measure word', 'container measure word'),\n",
    "        ('^Individual measure word', 'individual measure word'),\n",
    "        (r'\\[Commerce\\]', '[commerce]'),\n",
    "        (r'\\[Economics\\]', '[economics]'),\n",
    "        (r'\\[Formal\\]', '[formal]'),\n",
    "        (r'\\[Informal\\]', '[informal]'),\n",
    "        (r'\\[Medicine\\]', '[medicine]'),\n",
    "        (r'\\[Military\\]', '[military]'),\n",
    "        (r'\\[Music\\]', '[music]'),\n",
    "        (r'\\[Physics\\]', '[physics]'),\n",
    "        (r'\\[Polite\\]', '[polite]'),\n",
    "    ]:\n",
    "        text = re.sub(x, y, text)   \n",
    "    return text\n",
    "\n",
    "def fix_pos(text):\n",
    "    text = text.replace(' ', '').replace('；', '/').strip('()').strip()\n",
    "    for x, y in [('[;；]', '/'), (' ', ''), ('ADV', 'Adv'), ('VS', 'Vs'), ('[Pp]article', 'Ptc'), ('affix', 'Affix')]:\n",
    "        text = re.sub(x, y, text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02238ac9-fd82-47bb-b560-cb1793d68b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tocfl-202307.csv: 7517 terms, 7187 unique\n",
      "expanded/tocfl-202307.csv: 7848 terms, 7480 unique\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "expanded_rows = []\n",
    "prev_cat = ''\n",
    "\n",
    "for row in excel_df.fillna('').to_dict(orient='records'):\n",
    "    if row['ID'].endswith('-0001'):\n",
    "        prev_cat = ''\n",
    "    if row['Vocabulary'] == ' ' and row['ID'] == 'L2-0597' and row.get('Meaning') == 'Individual measure word for insects or animals.':\n",
    "        row['Vocabulary'] = '隻'\n",
    "        row['Pinyin'] = 'zhī'\n",
    "    row['Traditional'] = fix_vocabulary(row['Vocabulary'])\n",
    "    row['Pinyin'] = fix_pinyin(row['Pinyin'], row['Traditional'])\n",
    "    row['Variants'] = variants_to_json(get_variants(row['Traditional'], row['Pinyin'], row['POS']))\n",
    "    row['POS'] = fix_pos(row['POS'])\n",
    "    row['Simplified'] = to_simplified(row['Traditional'])\n",
    "    if 'Meaning' in row:\n",
    "        row['Meaning'] = fix_meaning(row['Meaning'])\n",
    "    if 'Category' in row:\n",
    "        row['Category'] = row['Category'].strip()\n",
    "        if row['Category']:\n",
    "            prev_cat = row['Category']\n",
    "        else:\n",
    "            row['Category'] = prev_cat\n",
    "    rows.append(row)\n",
    "\n",
    "    variants = json.loads(row['Variants']) if row['Variants'] else [{}]\n",
    "    for variant in variants:\n",
    "        var = dict(row)\n",
    "        var.update(variant)\n",
    "        expanded_rows.append(var)\n",
    "\n",
    "cols = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS']\n",
    "for col in ['Meaning', 'Category', 'Subcategory']:\n",
    "    if col in excel_df:\n",
    "        cols += [col]\n",
    "!rm -f \"{OUTPUT_CSV}\"\n",
    "tocfl_df = pd.DataFrame(rows)[cols + ['Variants']]\n",
    "tocfl_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print('%s: %d terms, %d unique' % (OUTPUT_CSV, len(tocfl_df), len(set(tocfl_df.Traditional))))\n",
    "assert list(tocfl_df.index) == list(sorted(tocfl_df.index))\n",
    "\n",
    "expanded_csv = 'expanded/' + OUTPUT_CSV\n",
    "expanded_df = pd.DataFrame(expanded_rows)[cols]\n",
    "expanded_df.to_csv(expanded_csv, index=False)\n",
    "assert list(expanded_df.index) == list(sorted(expanded_df.index))\n",
    "print('%s: %d terms, %d unique' % (expanded_csv, len(expanded_df), len(set(expanded_df.Traditional))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fbd5e-92ee-4565-a1a4-24cc5bfb2465",
   "metadata": {},
   "source": [
    "## Readings check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e3d3216-a8c3-465f-9ed5-8609d2409842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cccc.csv: 1197 terms (1344 with variants, 1312 unique)\n",
      "tocfl-20160215.csv: 7966 terms (8137 with variants, 7410 unique)\n",
      "tocfl-20160316.csv: 7966 terms (8136 with variants, 7409 unique)\n",
      "tocfl-20161230.csv: 7965 terms (8132 with variants, 7405 unique)\n",
      "tocfl-20170324.csv: 7965 terms (8132 with variants, 7405 unique)\n",
      "tocfl-20180419.csv: 7945 terms (8106 with variants, 7399 unique)\n",
      "tocfl-202307.csv: 7517 terms (7848 with variants, 7480 unique)\n",
      "tocfl.csv: 7517 terms (7848 with variants, 7480 unique)\n",
      "top-20100915.csv: 8868 terms (9061 with variants, 7457 unique)\n",
      "top-20111208.csv: 8013 terms (8177 with variants, 7427 unique)\n",
      "  L1-0496 姊姊 ['zǐzi'] vs. ['jiějiě', 'jiězǐ', 'jiějie', 'zǐjiě', 'zǐzǐ', 'zǐjie', 'jiejiě', 'jiezǐ', 'jiejie']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('../cedict/syllables.csv'):\n",
    "    readings_mp = {'一': set([]), '蹟': set(['jī']), '噢': set(['yǔ'])}\n",
    "    syll_df = pd.read_csv('../cedict/syllables.csv', dtype='str').fillna('')\n",
    "    for row in syll_df.itertuples():\n",
    "        readings_mp.setdefault(row.Traditional, set()).add(row.Pinyin.lower())\n",
    "    readings_mp = {x: set([y.strip().lower() for y in readings_mp[x] if y.strip()]) for x in readings_mp}\n",
    "    readings_mp['不'] -= set(['bú'])\n",
    "\n",
    "    def gen_readings(trad):\n",
    "        if trad == '':\n",
    "            yield ''\n",
    "        elif trad[0] not in readings_mp or ord(trad[0]) < 0x3E00:\n",
    "            yield from gen_readings(trad[1:])\n",
    "        else:\n",
    "            for x in readings_mp[trad[0]]:\n",
    "                for y in gen_readings(trad[1:]):\n",
    "                    yield x.lower() + (\"'\" if y and y[0] in 'aāáǎàeēéěèoōóǒò' else '') + y\n",
    "\n",
    "    for filename in sorted(glob.glob('expanded/*.csv')):\n",
    "        if 'tbcl' in filename: continue\n",
    "        if 'chars' in filename: continue\n",
    "        df0 = pd.read_csv(os.path.basename(filename), dtype='str')\n",
    "        df = pd.read_csv(filename, dtype='str').fillna('')\n",
    "        print('%s: %d terms (%d with variants, %d unique)' % (os.path.basename(filename), len(df0), len(df), len(set(df.Traditional))))\n",
    "        for row in df.itertuples():\n",
    "            trad, pinyin = row.Traditional, row.Pinyin.replace(' ', '')\n",
    "            readings = list(gen_readings(trad))\n",
    "            if pinyin.lower() not in readings:\n",
    "                print(' ', row.ID, row.Traditional, list(row._asdict().values())[4:5], 'vs.', readings)\n",
    "                #print('%s,%s,%s' % (row.Traditional, row.Pinyin, row.Pinyin.replace('yì', 'yī').replace('yí', 'yī').replace('bú', 'bù')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442acd0-f814-4c14-b6e1-aa078397526f",
   "metadata": {},
   "source": [
    "## Export as HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c60ced-bbc3-4330-aa95-4795d9cb5efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated html/z.html\n",
      "Generated html/tbcl-grammar.html\n",
      "Generated html/tbcl.html\n",
      "Generated html/tbcl-affix.html\n",
      "Generated html/tocfl-20160215.html\n",
      "Generated html/cccc.html\n",
      "Generated html/top-20111208.html\n",
      "Generated html/tocfl-202307.html\n",
      "Generated html/tocfl-cedict.html\n",
      "Generated html/top-20100915.html\n",
      "Generated html/tocfl-20170324.html\n",
      "Generated html/tocfl-20180419.html\n",
      "Generated html/tbcl-chars.html\n",
      "Generated html/tocfl.html\n",
      "Generated html/tocfl-20161230.html\n",
      "Generated html/tbcl-cedict.html\n",
      "Generated html/tocfl-20160316.html\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/Mottie/tablesorter\n",
    "# https://getmdl.io/components/index.html#tables-section\n",
    "# https://materialui.co/colors\n",
    "# gh-pages branch, publish to github.io\n",
    "\n",
    "import html\n",
    "\n",
    "HTML_HEAD = r'''<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\" />\n",
    "<title>{HTML_TITLE}</title>\n",
    "<style>\n",
    "html {\n",
    "  font-family: 'Roboto', sans-serif;\n",
    "  background: #999999;\n",
    "}\n",
    ".titlediv {\n",
    "  text-align: center;\n",
    "  padding: 10px 20px 10px 20px;\n",
    "  background-color: #ffffff;\n",
    "  font-size: 22pt;\n",
    "  box-shadow: rgba(0, 0, 0, 0.6) 0px 15px 25px;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  /*width: fit-content;*/\n",
    "  border-radius: 10px;\n",
    "  margin-bottom: 5px;\n",
    "}\n",
    "table.datatable {\n",
    "  background-color: #ffffff;\n",
    "  border-spacing: 0px; border-collapse: separate;\n",
    "  box-shadow: rgba(0, 0, 0, 0.6) 0px 15px 25px;\n",
    "  border: 1px solid #999999;\n",
    "}\n",
    "table.datatable th {\n",
    "  font-weight: bold;\n",
    "  border-style: none;\n",
    "  border-width: 1px;\n",
    "  color: #000000;\n",
    "  background-color: #ffffff;\n",
    "  text-align: left;\n",
    "  padding: 10px 10px 10px 10px;\n",
    "  border-bottom: 1px solid #aaaaaa;\n",
    "}\n",
    "table.datatable td {\n",
    "  padding: 5px 10px 5px 10px;\n",
    "  vertical-align: top;\n",
    "}\n",
    ".CID, .CLe/*vel*/, .CTr/*aditional*/, .CSi/*mplified*/, .CAf/*fix*/ { \n",
    "  white-space: nowrap;\n",
    "}\n",
    "tr.L0.odd  { background-color: #FFCDD2CC; }  /* red */\n",
    "tr.L0.even { background-color: #FFCDD266; }\n",
    "tr.L01.odd  { background-color: #FFCDD2CC; }  /* red */\n",
    "tr.L01.even { background-color: #FFCDD266; }\n",
    "tr.L02.odd  { background-color: #FFCCBCCC; }  /* deep orange */\n",
    "tr.L02.even { background-color: #FFCCBC66; }\n",
    "tr.L1.odd   { background-color: #FFECB3CC; }  /* amber */\n",
    "tr.L1.even  { background-color: #FFECB366; }\n",
    "tr.L2.odd   { background-color: #DCEDC8FF; }  /* light green */\n",
    "tr.L2.even  { background-color: #DCEDC866; }\n",
    "tr.L3.odd   { background-color: #B2DFDBCC; }  /* teal */\n",
    "tr.L3.even  { background-color: #B2DFDB66; }\n",
    "tr.L4.odd   { background-color: #BBDEFBCC; }  /* blue */\n",
    "tr.L4.even  { background-color: #BBDEFB66; }\n",
    "tr.L5.odd   { background-color: #D1C4E9CC; }  /* deep purple */\n",
    "tr.L5.even  { background-color: #D1C4E966; }\n",
    "tr.L6.odd   { background-color: #E1BEE7CC; }  /* purple */\n",
    "tr.L6.even  { background-color: #E1BEE766; }\n",
    "/*tbody:hover, tr:hover td { background-color: #ffffff; }*/\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div style=\"max-width: 90%; width: fit-content; margin-left: auto; margin-right: auto\">\n",
    "{TITLE_DIV}\n",
    "<table class=\"datatable\" style=\"margin-left: auto; margin-right: auto\">\n",
    "'''\n",
    "\n",
    "HTML_FOOTER ='''\n",
    "</table>\n",
    "</div>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "def gen_html(csv_path, html_path):\n",
    "    df = pd.read_csv(csv_path, dtype='str').fillna('')\n",
    "    #if 'Variants' in df: df = df[df.Variants != '']\n",
    "\n",
    "    csv_name = os.path.basename(csv_path)\n",
    "    if csv_name.startswith('tocfl-20') or \\\n",
    "       csv_name.startswith('tocfl-cedict') or \\\n",
    "       csv_name.startswith('tocfl-latest'):\n",
    "        df.insert(1, 'Level', df.ID.str.extract('^(L0-.|L[1-9])'))\n",
    "        df['Level'] = df.Level.map({\n",
    "            'L0-1': 'Novice 1',\n",
    "            'L0-2': 'Novice 2',\n",
    "            'L1': '1',\n",
    "            'L2': '2',\n",
    "            'L3': '3',\n",
    "            'L4': '4',\n",
    "            'L5': '5',\n",
    "        })\n",
    "\n",
    "    html_title = csv_name\n",
    "    title_div = ''\n",
    "    if csv_name.startswith('tocfl-20'):\n",
    "        html_title = 'TOCFL wordlist (' + csv_name[len('tocfl-'):].replace('.csv', '') + ')'\n",
    "        #title_div = f'<div class=\"titlediv\">{html_title}</div>'\n",
    "    elif csv_name.startswith('top-20'):\n",
    "        html_title = 'SC-TOP/TOCFL wordlist (' + csv_name[len('top-'):].replace('.csv', '') + ')'\n",
    "        #title_div = f'<div class=\"titlediv\">{html_title}</div>'\n",
    "    elif csv_name == 'cccc.csv':\n",
    "        html_title = \"CCCC (Children's Chinese Competency Certification) wordlist\"\n",
    "        #title_div = f'<div class=\"titlediv\">{html_title}</div>'\n",
    "\n",
    "    columns = [c for c in df.columns if c not in ['Variants']]\n",
    "    mm = {}\n",
    "\n",
    "    with open(html_path, 'w') as fp:\n",
    "        fp.write(HTML_HEAD.replace('{HTML_TITLE}', html_title).replace('{TITLE_DIV}', title_div))\n",
    "        fp.write('  <thead>\\n')\n",
    "        fp.write('    <tr>\\n')\n",
    "        for col in columns:\n",
    "            fp.write('      <th>%s</th>\\n' % col)\n",
    "        fp.write('    </tr>\\n')\n",
    "        fp.write('  </thead>\\n')\n",
    "        fp.write('  <tbody>\\n')\n",
    "        row_idx = 0\n",
    "        used_anc = set()\n",
    "\n",
    "        for row in df.to_dict(orient='records'):\n",
    "            if 'ID' in row and row['ID'].startswith('L'):\n",
    "                lev = row['ID'][:2]\n",
    "                if lev == 'L0':\n",
    "                    lev += row['ID'][3]\n",
    "            elif 'Level' in row:\n",
    "                lev = int(row['Level'][0])\n",
    "                lev = f'L{lev}'\n",
    "            else:\n",
    "                lev = ''\n",
    "            if ('tbcl' in csv_path) and lev:\n",
    "                lev = 'L' + str(int(lev[1]) - 1) + lev[2:]\n",
    "\n",
    "            if row.get('Variants'):\n",
    "                #vrows = [row]\n",
    "                vrows = []\n",
    "                for var in json.loads(row['Variants']):\n",
    "                    vrows.append(dict(row))\n",
    "                    vrows[-1].update(var)\n",
    "            else:\n",
    "                vrows = [row]\n",
    "\n",
    "            row_idx += 1\n",
    "            for vi, vrow in enumerate(vrows):\n",
    "                fp.write('  <tr class=\"%s %s\">\\n' % (lev, 'odd' if row_idx % 2 == 1 else 'even'))\n",
    "                for col in columns:\n",
    "                    val = vrow[col]\n",
    "                    if val is None:\n",
    "                        continue\n",
    "\n",
    "                    cls = 'C%s' % (col[:2])\n",
    "                    anchor = ''\n",
    "                    if val:\n",
    "                        if col in ('ID', 'Traditional', 'Simplified') and val not in used_anc:\n",
    "                            anchor = val\n",
    "                            used_anc.add(val)\n",
    "                        else:\n",
    "                            anchor = ''\n",
    "\n",
    "                    rowspan = ''\n",
    "                    colspan = ''\n",
    "                    if vi == 0 and len(set(v[col] for v in vrows)) == 1:\n",
    "                        if len(vrows) > 1:\n",
    "                            rowspan = f' rowspan=\"{len(vrows)}\"'\n",
    "                        for v in vrows:\n",
    "                            v[col] = None\n",
    "                        if col == 'Meaning' and 'Meaning Compounds Examples' in ' '.join(columns) \\\n",
    "                           and not row['Compounds'] and not row['Examples']:\n",
    "                            colspan = ' colspan=\"3\"'\n",
    "                            for v in vrows:\n",
    "                                v['Compounds'] = None\n",
    "                                v['Examples'] = None\n",
    "                    fp.write(f'    <td class=\"{cls}\"{rowspan}{colspan}>')\n",
    "\n",
    "                    if val:\n",
    "                        if anchor:\n",
    "                            fp.write(f'<a name=\"{anchor}\">{val}</a>')\n",
    "                        elif col == 'MOE':\n",
    "                            for id in val.split():\n",
    "                                fp.write(f'<a href=\"https://dict.concised.moe.edu.tw/dictView.jsp?ID={id}\">{id}</a> ')\n",
    "                        elif '<br>' in val:\n",
    "                            val = '<br>'.join([html.escape(s) for s in val.split('<br>')])\n",
    "                            fp.write(val)\n",
    "                        else:\n",
    "                            fp.write(html.escape(val))\n",
    "                    fp.write(f'</td>\\n')\n",
    "\n",
    "                fp.write('  </tr>\\n')\n",
    "\n",
    "        fp.write('  </tbody>\\n')\n",
    "        fp.write(HTML_FOOTER)\n",
    "    print(f'Generated {html_path}')\n",
    "\n",
    "!mkdir -p html/\n",
    "for fn in glob.glob('*.csv'):\n",
    "    if fn == 'errata.csv': continue\n",
    "    gen_html(fn, 'html/' + fn.replace('.csv', '.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25afa3-f8ee-4d88-af18-51a64660f53e",
   "metadata": {},
   "source": [
    "## Export as Pleco user dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ded8f20-785f-44dd-8ad7-54305e8902e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 788\n",
      "-rw-r--r-- 1 jovyan users   94605 Apr 15 00:37 cccc-pleco.txt\n",
      "-rw-r--r-- 1 jovyan users 1121374 Apr 15 00:19 tbcl-pleco.txt\n",
      "-rw-r--r-- 1 jovyan users  414516 Apr 15 00:37 tocfl-pleco.txt\n",
      "-rw-r--r-- 1 jovyan users  634408 Apr 15 00:37 top-pleco.txt\n"
     ]
    }
   ],
   "source": [
    "EAC1_TAG = '\\uEAC1\\uEC00\\uEC00\\uECCC\\uEC99'  # tag color, #00cc99 green\n",
    "\n",
    "def gen_pleco(input_fn, output_fn, title, tag):\n",
    "    with open(output_fn, 'w') as fout:\n",
    "        last_header = ''\n",
    "        for row in pd.read_csv(input_fn, dtype='str').fillna('').to_dict(orient='records'):\n",
    "            cefr = {'0': 'pre-A1', '1': 'A1', '2': 'A2', '3': 'B1', '4': 'B2', '5': 'C1+'}[row['ID'][1]]\n",
    "            if 'cccc' in input_fn:\n",
    "                cefr = {'1': 'pre-A1', '2': 'A1', '3': 'A2'}[row['ID'][1]]\n",
    "                header = f\"//{title}/Level {row['ID'][1]} ({cefr})\"\n",
    "            elif row['ID'].startswith('L0'):\n",
    "                header = f\"//{title}/Novice {row['ID'][3]} ({cefr})\"\n",
    "            else:\n",
    "                header = f\"//{title}/Level {row['ID'][1]} ({cefr})\"\n",
    "\n",
    "            if header != last_header:\n",
    "                last_header = header\n",
    "                fout.write(header + '\\n')\n",
    "\n",
    "            variants = json.loads(row['Variants']) if row['Variants'] else [{}]\n",
    "            for variant in variants:\n",
    "                var = dict(row)\n",
    "                var.update(variant)\n",
    "                defn = ' '.join([\n",
    "                    f\"{row['Traditional']} [{row['Pinyin']}]\\uEAB1\" if row['Variants'] else '',\n",
    "                    f\"({row['POS']})\" if row.get('POS') else '',\n",
    "                    f\"{row['Meaning']}\" if row.get('Meaning') else '',\n",
    "                    f\"{EAC1_TAG}[{tag}{row['ID'][1]}]\\uEAC2\"\n",
    "                ])\n",
    "                defn = re.sub(r'\\s+', ' ', defn).replace('\\uEAB1 ', '\\uEAB1').strip()\n",
    "                key = f\"{var['Simplified']}[{var['Traditional']}]\\t{var['Pinyin']}\"\n",
    "                fout.write(f'{key}\\t{defn}\\n')\n",
    "\n",
    "gen_pleco('top-20111208.csv', 'pleco/top-pleco.txt', 'TOP 20111208', 'TOP')\n",
    "gen_pleco('tocfl.csv', 'pleco/tocfl-pleco.txt', 'TOCFL 2023', 'T')\n",
    "gen_pleco('cccc.csv', 'pleco/cccc-pleco.txt', 'CCCC 2022', 'CCCC')\n",
    "!ls -l pleco/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0c648-774d-4384-aa99-806e62ab3632",
   "metadata": {},
   "source": [
    "## Join with CC-CEDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "944e72b6-c656-4426-81b4-6cf6eb69b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNTONE_MP = {\n",
    "    'a': 'a', 'ā': 'a', 'á': 'a', 'ǎ': 'a', 'à': 'a',\n",
    "    'e': 'e', 'ē': 'e', 'é': 'e', 'ě': 'e', 'è': 'e',\n",
    "    'o': 'o', 'ō': 'o', 'ó': 'o', 'ǒ': 'o', 'ò': 'o',\n",
    "    'i': 'i', 'ī': 'i', 'í': 'i', 'ǐ': 'i', 'ì': 'i',\n",
    "    'u': 'u', 'ū': 'u', 'ú': 'u', 'ǔ': 'u', 'ù': 'u',\n",
    "    'ü': 'ü', 'ǖ': 'ü', 'ǘ': 'ü', 'ǚ': 'ü', 'ǜ': 'ü'\n",
    "}\n",
    "\n",
    "# Check if pinyin from the list (py1) matches cedict's (py2)\n",
    "def pinyin_matches(py1, py2, hz='', untone=False):\n",
    "    py1 = py1.lower()\n",
    "    py2 = py2.lower()\n",
    "    i, j = 0, 0\n",
    "    while i < len(py1) or j < len(py2):\n",
    "        a = ''\n",
    "        if i < len(py1):\n",
    "            a = py1[i]\n",
    "            if a in \"-',/() \":\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        b = ''\n",
    "        if j < len(py2):\n",
    "            b = py2[j]\n",
    "            if b in \"-',/() \":\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "        match = (a == b)\n",
    "        match |= untone and (UNTONE_MP.get(a, a) == b or a == UNTONE_MP.get(b, b))\n",
    "        if match:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    return i == len(py1) and j == len(py2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2427dd38-28d8-4202-90c6-e0c8397fbae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambigous simplified: {'ID': 'L0-2234', 'Traditional': '著', 'Simplified': '着', 'Pinyin': 'zhe', 'POS': 'Ptc', 'Variants': ''} ce {'著', '着'} cc 着\n",
      "Ambigous simplified: {'ID': 'L2-0171', 'Traditional': '乾', 'Simplified': '干', 'Pinyin': 'gān', 'POS': 'Vp', 'Variants': ''} ce {'乾', '干'} cc 干\n",
      "No entry for {'ID': 'L2-0178', 'Traditional': '汙染', 'Simplified': '污染', 'Pinyin': 'wūrǎn', 'POS': 'V', 'Variants': ''}\n",
      "No entry for {'ID': 'L2-0292', 'Traditional': '月台', 'Simplified': '月台', 'Pinyin': 'yuètái', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L3-0326', 'Traditional': '還要', 'Simplified': '还要', 'Pinyin': 'háiyào', 'POS': 'Adv', 'Variants': ''}\n",
      "No entry for {'ID': 'L3-0335', 'Traditional': '好了', 'Simplified': '好了', 'Pinyin': 'hǎole', 'POS': 'Ptc', 'Variants': ''}\n",
      "No entry for {'ID': 'L3-0809', 'Traditional': '說起來', 'Simplified': '说起来', 'Pinyin': 'shuōqǐlái', 'POS': 'Adv', 'Variants': ''}\n",
      "Ambigous simplified: {'ID': 'L3-1144', 'Traditional': '著', 'Simplified': '着', 'Pinyin': 'zhuó', 'POS': 'V', 'Variants': ''} ce {'著', '着'} cc 着\n",
      "No entry for {'ID': 'L3-1157', 'Traditional': '走走', 'Simplified': '走走', 'Pinyin': 'zǒuzǒu', 'POS': 'Vi', 'Variants': ''}\n",
      "No entry for {'ID': 'L4-1238', 'Traditional': '鈕扣', 'Simplified': '钮扣', 'Pinyin': 'niǔkòu', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L4-1646', 'Traditional': '算帳', 'Simplified': '算帐', 'Pinyin': 'suànzhàng', 'POS': 'V-sep', 'Variants': ''}\n",
      "Simplified diff: {'ID': 'L4-1661', 'Traditional': '牠', 'Simplified': '它', 'Pinyin': 'tā', 'POS': 'N', 'Variants': ''} ce {'牠'} cc 牠\n",
      "Ambigous simplified: {'ID': 'L4-1838', 'Traditional': '閒', 'Simplified': '闲', 'Pinyin': 'xián', 'POS': 'Vs', 'Variants': ''} ce {'閒', '闲'} cc 闲\n",
      "No entry for {'ID': 'L4-1941', 'Traditional': '吸菸', 'Simplified': '吸烟', 'Pinyin': 'xīyān', 'POS': 'V-sep', 'Variants': ''}\n",
      "No entry for {'ID': 'L5-0954', 'Traditional': '艱鉅', 'Simplified': '艰巨', 'Pinyin': 'jiānjù', 'POS': 'Vs', 'Variants': ''}\n",
      "Simplified diff: {'ID': 'L5-1028', 'Traditional': '藉口', 'Simplified': '借口', 'Pinyin': 'jièkǒu', 'POS': 'N', 'Variants': ''} ce {'藉口'} cc 借口\n",
      "No entry for {'ID': 'L5-1523', 'Traditional': '奇蹟', 'Simplified': '奇迹', 'Pinyin': 'qíjī', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L5-1935', 'Traditional': '台階', 'Simplified': '台阶', 'Pinyin': 'táijiē', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L5-2106', 'Traditional': '汙染', 'Simplified': '污染', 'Pinyin': 'wūrǎn', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L5-2155', 'Traditional': '鮮艶/鮮豔', 'Simplified': '鲜艳', 'Pinyin': 'xiānyàn', 'POS': 'Vs', 'Variants': '[{\"Traditional\": \"鮮艶\", \"Simplified\": \"鲜艳\", \"Pinyin\": \"xiānyàn\"}, {\"Traditional\": \"鮮豔\", \"Simplified\": \"鲜艳\", \"Pinyin\": \"xiānyàn\"}]'}\n",
      "No entry for {'ID': 'L5-2373', 'Traditional': '意識到', 'Simplified': '意识到', 'Pinyin': 'yìshìdào', 'POS': 'Vpt', 'Variants': ''}\n",
      "Ambigous simplified: {'ID': 'L5-2445', 'Traditional': '於', 'Simplified': '于', 'Pinyin': 'yú', 'POS': 'Prep', 'Variants': ''} ce {'于', '於'} cc 于\n",
      "Ambigous simplified: {'ID': 'L5-2447', 'Traditional': '餘', 'Simplified': '余', 'Pinyin': 'yú', 'POS': 'N/Vst', 'Variants': ''} ce {'余', '馀'} cc 余\n",
      "No entry for {'ID': 'L5-2692', 'Traditional': '轉帳', 'Simplified': '转帐', 'Pinyin': 'zhuǎnzhàng', 'POS': 'V-sep', 'Variants': ''}\n"
     ]
    }
   ],
   "source": [
    "tocfl_df = pd.read_csv('tocfl.csv', dtype='str').fillna('')\n",
    "top_df = pd.read_csv('top-20111208.csv', dtype='str').fillna('')\n",
    "cedict_df = pd.read_csv('../cedict/cedict.csv', dtype='str').fillna('')\n",
    "cedict_idx_mp = cedict_df.assign(idx=cedict_df.index).groupby('Traditional').idx.apply(list)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for row in tocfl_df.to_dict(orient='records'):\n",
    "    pinyin_set = set([row['Pinyin']])\n",
    "    matches = cedict_idx_mp.get(row['Traditional'], [])\n",
    "    if len(matches) == 0 and row['Variants']:\n",
    "        variants = json.loads(row['Variants']) if row['Variants'] else [{}]\n",
    "        for variant in variants:\n",
    "            matches.extend(cedict_idx_mp.get(variant['Traditional'], []))\n",
    "            pinyin_set.add(variant['Pinyin'])\n",
    "\n",
    "    matches = list(sorted(set(matches)))\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        print('No entry for %s' % row)\n",
    "    else:\n",
    "        # Prioritize pronunciation matches, downpriorize names and variants\n",
    "        # TODO: match based on taiwanese pronunciation\n",
    "        if len(matches) > 1:\n",
    "            matches.sort(key=lambda i: (\n",
    "                -int(any(pinyin_matches(py, cedict_df.Pinyin[i], untone=False) for py in pinyin_set))\n",
    "                -int(any(pinyin_matches(py, cedict_df.Pinyin[i], untone=True) for py in pinyin_set))\n",
    "                +10*int(re.match('^(variant|used in)', cedict_df.Definitions[i]) is not None)\n",
    "                +100*int(cedict_df.Pinyin[i][0].isupper())\n",
    "                +1000*int(cedict_df.Pinyin[i][0].isupper())*int(re.match('^surname ', cedict_df.Definitions[i]) is not None)\n",
    "            ))\n",
    "\n",
    "        ce_simp = set([cedict_df.Simplified[i] for i in matches])\n",
    "        cc_simp = opencc_tw2s.convert(row['Traditional'])\n",
    "        if not row['Variants'] and ce_simp:\n",
    "            if row['Simplified'] not in ce_simp:\n",
    "                print('Simplified diff:', row, 'ce', ce_simp, 'cc', cc_simp)\n",
    "            if len(ce_simp) > 1:\n",
    "                print('Ambigous simplified:', row, 'ce', ce_simp, 'cc', cc_simp)\n",
    "\n",
    "        defs = []\n",
    "        for i in matches:\n",
    "            py1 = list(pinyin_set)[0] if len(pinyin_set) == 1 else ''\n",
    "            defn = cedict_df.Definitions[i]\n",
    "            if defn.startswith('surname ') and cedict_df.Pinyin[i][0].isupper() and len(defs) > 0:\n",
    "                continue\n",
    "            if len(defs) == 0 and cedict_df.Pinyin[i][0].isupper() and row['Pinyin'][0].islower():\n",
    "                print('%s,%s,%s' % (row['Traditional'], row['Pinyin'], row['Pinyin'][0].upper() + row['Pinyin'][1:]))\n",
    "            defn = re.sub(r'/CL:個\\|个\\[ge4\\](|/.*)$', r'\\1', defn)  # uninformative\n",
    "            defn = defn.replace(' (CL:個|个[ge4])/', '/')\n",
    "            if row['Variants']:\n",
    "                defn = '%s [%s] %s' % (cedict_df.Traditional[i], cedict_df.Pinyin[i], defn)\n",
    "            elif not pinyin_matches(py1, cedict_df.Pinyin[i], untone=False):\n",
    "                defn = '[%s] %s' % (cedict_df.Pinyin[i], defn)\n",
    "            defs.append(defn)\n",
    "\n",
    "        row['Meaning'] = '<br> '.join(defs)\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "merged_df = pd.DataFrame(rows)\n",
    "merged_df.to_csv('tocfl-cedict.csv', index=False)\n",
    "\n",
    "# diffs mostly due to variants chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3edf60-e80b-4c1e-bd10-9efaa9ff6e1e",
   "metadata": {},
   "source": [
    "## Generate Anki deck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b63a0-7838-483f-848f-d324ea27019e",
   "metadata": {},
   "source": [
    "*Taiwan TOCFL 2023 wordlist with audio (Traditional)*\n",
    "\n",
    "Complete wordlist of TOCFL (Test of Chinese as a Foreign Language), a taiwanese equivalent of HSK.\n",
    "\n",
    "Parsed from official excel sheets from [TOCFL](https://tocfl.edu.tw/) website. This is a new 2022/2023 version ([8000zhuyin_202307.zip](https://tocfl.edu.tw/assets/files/vocabulary/8000zhuyin_202307.zip)) of the list with 7517 entries (previous 2018 list had 7945 entries.)\n",
    "\n",
    "Columns:\n",
    "  * `ID`: term's level + index (row number in original excel file which has one sheet per level):\n",
    "    * `L0-1nnn` = Novice 1 (準備級一級), `L0-2nnn` = Novice 2 (準備級二級), both pre-A1, `L1-nnnn`..`L5-nnnn` = Level 1..5 (入門級/基礎級/進階級/高階級/流利級) = CEFR A1/A2/B1/B2/C1+.\n",
    "    * Levels are also added as tags.\n",
    "  * `Traditional`: term in traditional characters per TOCFL.\n",
    "  * `Simplified`: term converted to simplified characters.\n",
    "  * `Pinyin`: pinyin with diacritics, slightly cleaned up from TOCFL sheets, e.g. missing apostrophes added and a few clear errors corrected. Tone changes are not indicated.\n",
    "  * `POS`: part of speech, `/`-separated. See [description](https://tocfl.edu.tw/assets/files/vocabulary/8000_description_202204.pdf) on TOCFL website for the meaning of abbreviations (202204 list is essentially same)\n",
    "  * `Meaning`: definitions from [CC-CEDICT](https://www.mdbg.net/chinese/dictionary?page=cedict) for convenience. Note it mainly lists mainland pronunciations which may differ from taiwanese in some cases. [CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) licensed.\n",
    "  * `Audio`: good quality neural TTS audio with a taiwanese mandarin voice.\n",
    "  * `Variants`: for entries where TOCFL gives multiple variants of a term, an expanded disambiguated list as a JSON list of objects with alternatives column values. If using this deck for an automatic analysis (such as merging with other sources or your anki decks), you might find this field useful as the original source is inconsistent in formatting variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cc9e4dd-fb8a-4bac-9b26-a4c77024362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 154530539 Apr 15 00:37 tocfl.apkg\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tocfl-cedict.csv', dtype='str').fillna('')\n",
    "df['Audio'] = ''\n",
    "\n",
    "!mkdir -p downloads/media\n",
    "if os.path.exists('downloads/media/_MoeStandardKai.ttf'):\n",
    "    !cp -f ../downloads/fonts/MoeStandardKai.ttf downloads/media/_MoeStandardKai.ttf\n",
    "if os.path.exists('../Anki2/hypertts.tsv'):\n",
    "    tts_mp = pd.read_csv('../Anki2/hypertts.tsv', sep='\\t').set_index('Text').Hash.to_dict()\n",
    "    for row in df.itertuples():\n",
    "        text = json.loads(row.Variants)[0]['Traditional'] if row.Variants else row.Traditional\n",
    "        dst = 'downloads/media/tocfl-tts-%s.mp3' % text\n",
    "        if not os.path.exists(dst) and text in tts_mp:\n",
    "            shutil.copy('../Anki2/tts/collection.media/hypertts-%s.mp3' % tts_mp[text], dst)\n",
    "        df.loc[row.Index, 'Audio'] = '[sound:tocfl-tts-%s.mp3]' % text\n",
    "\n",
    "cols = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS', 'Meaning', 'Variants', 'Audio']\n",
    "\n",
    "model = genanki.Model(\n",
    "    1698016000,\n",
    "    'TOCFL',\n",
    "    fields=[{'name': c} for c in cols],\n",
    "    templates=[{\n",
    "        'name': 'TOCFL',\n",
    "        'qfmt': open('../dangdai/dangdai-qfmt.html').read(),\n",
    "        'afmt': open('../dangdai/dangdai-afmt.html').read().replace(\n",
    "            'if (pinyinEl && hanziEl)',\n",
    "            'if (pinyinEl && hanziEl {{#Variants}}&& false{{/Variants}})'),\n",
    "    }],\n",
    "    css=open('../dangdai/dangdai.css').read(),\n",
    ")\n",
    "\n",
    "deck = genanki.Deck(1698016001, name='tocfl')\n",
    "\n",
    "for row in df.reset_index().to_dict(orient='records'):\n",
    "    note = genanki.Note(\n",
    "        model=model,\n",
    "        fields=[row[c] for c in cols],\n",
    "        guid=genanki.guid_for('tocfl', row['ID']),\n",
    "        tags=[row['ID'][:2]],\n",
    "    )\n",
    "    deck.add_note(note)\n",
    "\n",
    "!rm -f tocfl.apkg\n",
    "genanki.Package(deck, media_files=glob.glob('downloads/media/*')).write_to_file('tocfl.apkg')\n",
    "!ls -l tocfl.apkg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
